### HW 5

* Скачал манифесты по ссылкам

* Создал Opaque secret с двумя ключами значениями "зашифрованными" в base64:
```
base64 | minio
base64 | minio123
```

* Применил манифест secret'a.
```
kubectl apply -f minio-secret.yaml
```

* Изменил значения полей обьявнения переменных окружения в поде StatefulSet, таким образом, чтобы они брались из secret.

* Применил манифесты StatefulSet и PV/PVC
```
$ kubectl apply -f minio-statefulset-secret.yaml
statefulset.apps/minio created
$ kubectl apply -f minio-headless-service.yaml
service/minio created
```

* Проверил созданные ресурсы.  
```
$ kubectl get statefulsets
NAME    READY   AGE
minio   1/1     30s
$ kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
minio-0   1/1     Running   0          39s
$ kubectl get pvc
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-minio-0   Bound    pvc-23dfbcbd-47c4-44d5-8583-e5e52ba7874a   10Gi       RWO            standard       48s
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE
pvc-23dfbcbd-47c4-44d5-8583-e5e52ba7874a   10Gi       RWO            Delete           Bound    default/data-minio-0   standard                59s
```

* Исследовал созданные ресурсы с помощью kubectl describe.

### HW 4

* Создал Deployment, исследовали механизм livenessProbe/readinessProbe в различных вариациях.

* Создал Service с ClusterIP.

* Модифицировал конфигурационные файлы minikube для активации механизма ipvs.

* Установил плагин MetalLB.

* Создал сервис с типом LoadBalancer.

* Установили плагин Ingress-Nginx.

* Создал "безголовый сервис" и настроили для него доступ через Ingress.

* Выполнил все задания с *.

### HW 3

* Создал два аккаунта с полными правами и без прав соответсвенно.
 
* Создал нэймспейс с ro аккаунтом. 

* Создал неймспейс и два аккаунта с различными правами.

### HW 2

#### Ответы на вопросы заданные в задании и предлагающие найти ответы самостоятельно:

* После запуска kubectl apply -f frontend-replicaset.yaml --dry-run=client получаем ошибку: missing required field "selector".
  Очевидно, в манифесте не хватает селектора, связывающего pod с управляющим контроллером. Зададим его.

* ReplicaSet не управляет версиями контейнеров, этот контроллер следит за тем, 
  чтобы фактическое количество работающих подов соотвествовало тому количеству, 
  которое прописано в манифесте, для отслеживания версий контейнеров в подах существует контроллер Deployment.

* Для того чтобы node-exporter был развёрнут так же на master нодах, необходимо добавить в манифест указание шедулеру игнорировать толерантность нод для этого пода:  
     ```
     tolerations:
     - effect: NoSchedule
       operator: Exists
     ```

### HW 1
#### 1. Подготовка локального кластера k8s на рабочей станции.
* Установлен VirtualBox на рабочую станцию.
* Создана виртуальная машина с ubuntu-server 20.04
* Установлен Docker 19.03.14
* Установлен minikube
* Установлен kubectl c bash completition
* Выполнено исследование причин и механизмов восстановления служебных компонентов k8s при их удалении:
   + kublet - это агент на ноде (не под) в котором зашиты алгоритмы восстановления основных компонентов (etcd, apiserver, controller, scheduler).
     При их падении, он определит их состояние, применяя обычные механизмы liveness check, и восстанавит автоматически.
   + Состоянием остальных компонентов управляет контроллер в зависимости от шаблона который к ним прикреплён:
     - core-dns - это Pod управляемый шаблоном ReplicaSet контроллера.
     - kube-proxy - это Pod управляемый шаблоном DaemonSet контроллера.

#### 2. Подготовка и создание сервиса в minikube
* Создан docker образ на основе alpine с установленным внутри nginx, порт VirtualHost по умолчанию задан 8000.
* Выполнен login на Dockerhub в качестве docker registry и сделан pull образа stitrace/nginx-kubernates-intro с тегом 0.0.1
* Создан и применён манифест пода для запуска этого контейнера в кластере. Исследованы возможности мониторинга его состояния с помощью kubectl.
* Модифицирован манифест путём добавления init контейнера busybox, который скачивает скрипт создания индексной страницы для вебсервера основного контейнера пода,
  созданая страница "шарится" в контейнер с вебсервером через общую примонтированную к обоим контейнерам папку.

#### 3. Запуск создание образа контейнера и запуск микросервиса в поде кластера.
* Скачан пример микросервиса с github.
* Построен контейнер из Dockerfile в его репозитории.
* Контейнер помещён в регистри dockerhub stitrace/hipster-shop-frontend с тэгом 0.0.1
* Сгенерирован манифест с помощью ad-hoc kubectl для запуска этого образа в кластере. Исследованы причины ошибки при его запуске.
* Манифест модифицирован добавлением секции env, которая описывает, необходимые для запуска внутри контейнера кода микросервиса, переменные окружения.
* Убедились что pod с контейнером микросервиса был успешно создан и находится в состоянии Running с помощью kubectl describe и kubectl get pod.

